{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "flownet3d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oltMOCo4gkD0"
      },
      "source": [
        "# Week 4: Scene Flow Estimation\n",
        "\n",
        "If you are running on Colab,\n",
        "* Go to runtime -> change runtime type -> select \"GPU\" as the hardware accelerator. \n",
        "\n",
        "This week's assignment has two parts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffiRr-EEgkD9"
      },
      "source": [
        "## Part A\n",
        "\n",
        "In the first part, you'll annotate the code below for [FlowNet3D](https://arxiv.org/abs/1806.01411). What does it mean to annotate a notebook?\n",
        "\n",
        "The original implementation of FlowNet3D is in TensorFlow: [tf-code](https://github.com/xingyul/flownet3d). Someone has implemented it in PyTorch and provided the notebook below: [pytorch-code](https://github.com/multimodallearning/flownet3d.pytorch). This is great but the notebook is not very helpful for someone trying to match it to the paper.\n",
        "\n",
        "When it is done right, it looks like this: [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html). Can you provide a similar encapsulation for the code below by putting relevant parts of the paper before the code segment? Feel free to divide code segments further and copy text or figures from the paper but make sure that it matches the code and furthermore explains what is done in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTNFELgpDhcr"
      },
      "source": [
        "# 3. Problem Definition\n",
        "<img align=\"right\" width=\"400\" height=\"200\" src=\"https://drive.google.com/uc?id=1i0LZbqrfOnsCYJLbHBrfAPSARPD9HRUV\">\n",
        "\n",
        "\n",
        "Input to the network are two sets of points sampled from a dynamic 3D scene, at two consecutive time frames: $\\mathcal{P}=\\left\\{x_{i} \\mid i=\\right.$ $\\left.1, \\ldots, n_{1}\\right\\}($ point cloud 1$)$ and $\\mathcal{Q}=\\left\\{y_{j} \\mid j=1, \\ldots, n_{2}\\right\\}$\n",
        "(point cloud 2), where $x_{i}, y_{j} \\in \\mathbb{R}^{3}$ are $X Y Z$ coordinates of individual points.\n",
        "\n",
        "Now consider the physical point under a sampled point $x_{i}$ moves to location $x_{i}^{\\prime}$ at the second frame, then the translational motion vector of the point is $d_{i}=x_{i}^{\\prime}-x_{i} .$ The goal is, given $\\mathcal{P}$ and $\\mathcal{Q},$ to recover the scene flow for every sampled point in the first frame: $\\mathcal{D}=\\left\\{d_{i} \\mid i=1, \\ldots, n_{1}\\right\\}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5SkU4bWWAjy"
      },
      "source": [
        "# 4. FlowNet3D Architecture\n",
        "The model has three key modules for (1) point feature learning,\n",
        "(2) point mixture, and (3) flow refinement.\n",
        "\n",
        "## 4.1. Hierarchical Point Cloud Feature Learning\n",
        "<img align=\"right\" width=\"500\" height=\"200\" src=\"https://drive.google.com/uc?id=1vzPt7Y3DJl6to_HlJEGDNH37aOI8CCu8\">\n",
        "\n",
        "set conv layer is used to learn deep point cloud features. As shown in the figure, a set conv layer takes a point cloud with $n$ points, each point $p_{i}=\\left\\{x_{i}, f_{i}\\right\\}$ with its $X Y Z$ coordinates $x_{i} \\in \\mathbb{R}^{3}$ and its feature $f_{i} \\in \\mathbb{R}^{c}(i=1, \\ldots, n)$\n",
        "and outputs a sub-sampled point cloud with $n^{\\prime}$ points, where each point $p_{j}^{\\prime}=\\left\\{x_{j}^{\\prime}, f_{j}^{\\prime}\\right\\}$ has its $X Y Z$ coordinates $x_{j}^{\\prime}$ and an updated point feature $f_{j}^{\\prime} \\in \\mathbb{R}^{c^{\\prime}}\\left(j=1, \\ldots n^{\\prime}\\right)$\n",
        "\n",
        "the layer firstly samples $n'$ regions from the input points with farthest point sampling (with region centers as $x'_j$):\n",
        "\n",
        "```\n",
        "class Sample(nn.Module):\n",
        "    def __init__(self, num_points):\n",
        "        super(Sample, self).__init__()\n",
        "        self.num_points = num_points\n",
        "        \n",
        "    def forward(self, points):\n",
        "        new_points_ind = furthest_point_sampling(points.permute(0, 2, 1).contiguous(), self.num_points)\n",
        "        new_points = fps_gather_by_index(points, new_points_ind)\n",
        "        return new_points\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M466xlz6h4g_"
      },
      "source": [
        "Then for each region (defined by a radius neighborhood specified by radius $r)$, it extracts its local feature with the following symmetric function\n",
        "$$\n",
        "f_{j}^{\\prime}=\\operatorname{MAX}_{\\left\\{i \\mid\\left\\|x_{i}-x_{j}^{\\prime}\\right\\| \\leq r\\right\\}}\\left\\{h\\left(f_{i}, x_{i}-x_{j}^{\\prime}\\right)\\right\\}\n",
        "$$\n",
        "where $h: \\mathbb{R}^{c+3} \\rightarrow \\mathbb{R}^{c^{\\prime}}$ is a non-linear function (realized as a multi-layer perceptron) with concatenated $f_{i}$ and $x_{i}-x_{j}^{\\prime}$ as inputs, and MAX is element-wise max pooling.\n",
        "\n",
        "```\n",
        "class Group(nn.Module):\n",
        "    def __init__(self, radius, num_samples, knn=False):\n",
        "        super(Group, self).__init__()\n",
        "        \n",
        "        self.radius = radius\n",
        "        self.num_samples = num_samples\n",
        "        self.knn = knn\n",
        "        \n",
        "    def forward(self, points, new_points, features):\n",
        "        if self.knn:\n",
        "            dist = pdist2squared(points, new_points)\n",
        "            ind = dist.topk(self.num_samples, dim=1, largest=False)[1].int().permute(0, 2, 1).contiguous()\n",
        "        else:\n",
        "            ind = ball_query(self.radius, self.num_samples, points.permute(0, 2, 1).contiguous(),\n",
        "                             new_points.permute(0, 2, 1).contiguous(), False)\n",
        "        grouped_points = group_gather_by_index(points, ind)\n",
        "        grouped_points -= new_points.unsqueeze(3)\n",
        "        grouped_features = group_gather_by_index(features, ind)\n",
        "        new_features = torch.cat([grouped_points, grouped_features], dim=1)\n",
        "        return new_features\n",
        "\n",
        "class SetConv(nn.Module):\n",
        "    def __init__(self, num_points, radius, num_samples, in_channels, out_channels):\n",
        "        super(SetConv, self).__init__()\n",
        "        \n",
        "        self.sample = Sample(num_points)\n",
        "        self.group = Group(radius, num_samples)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [in_channels+3, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points, features):\n",
        "        new_points = self.sample(points)\n",
        "        new_features = self.group(points, new_points, features)\n",
        "        new_features = self.conv(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        return new_points, new_features\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJuCo6jHtqcl"
      },
      "source": [
        "## 4.2. Point Mixture with Flow Embedding Layer\n",
        "\n",
        "<img align=\"right\" width=\"400\" height=\"200\" src=\"https://drive.google.com/uc?id=1NtBZ9xt-gS0HZ4hNrzDtt3jF6HGsaACk\">\n",
        "\n",
        "To mix two point clouds they rely on a new flow embedding\n",
        "layer shown in the figure. The layer learns a flow embedding\n",
        "for each point in the first frame. the flow embedding layer takes a pair of point clouds: $\\left\\{p_{i}=\\left(x_{i}, f_{i}\\right)\\right\\}_{i=1}^{n_{1}}$ and $\\left\\{q_{j}=\\left(y_{j}, g_{j}\\right)\\right\\}_{j=1}^{n_{2}}$ where\n",
        "each point has its $X Y Z$ coordinate $x_{i}, y_{j} \\in \\mathbb{R}^{3},$ and a feature vector $f_{i}, g_{j} \\in \\mathbb{R}^{c} .$ \n",
        "They use a neural layer to aggregate flow votes from\n",
        "all the neighboring,\n",
        "\n",
        "$$\n",
        "e_{i}=\\operatorname{MAX}_{\\left\\{j \\mid\\left\\|_{j}-x_{i}\\right\\| \\leq r\\right\\}}\\left\\{h\\left(f_{i}, g_{j}, y_{j}-x_{i}\\right)\\right\\}\n",
        "$$\n",
        "\n",
        "```\n",
        "class FlowEmbedding(nn.Module):\n",
        "    def __init__(self, num_samples, in_channels, out_channels):\n",
        "        super(FlowEmbedding, self).__init__()\n",
        "        \n",
        "        self.num_samples = num_samples\n",
        "        \n",
        "        self.group = Group(None, self.num_samples, knn=True)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [2*in_channels+3, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        new_features = self.group(points2, points1, features2)\n",
        "        new_features = torch.cat([new_features, features1.unsqueeze(3).expand(-1, -1, -1, self.num_samples)], dim=1)\n",
        "        new_features = self.conv(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        return new_features\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrGw25z0wOL"
      },
      "source": [
        "## 4.3. Flow Refinement with Set Upconv Layer\n",
        "<img align=\"right\" width=\"500\" height=\"200\" src=\"https://drive.google.com/uc?id=1HV-QpBJsBzYVEbbtSSu8kIC5jfIbtG1S\">\n",
        "The up-sampling step is achieved by a learnable new layer\n",
        "â€“ the set upconv layer, which learns to propagate and refine\n",
        "the embeddings in an informed way. The inputs to the layer are source points $\\left\\{p_{i}=\\right.$ $\\left.\\left\\{x_{i}, f_{i}\\right\\} \\mid i=1, \\ldots, n\\right\\},$ and a set of target point coordinates $\\left\\{x_{j}^{\\prime} \\mid j=1, \\ldots, n^{\\prime}\\right\\}$ which are locations we want to propagate the source point features to.\n",
        "\n",
        "Interestingly, just like in 2D convolutions in images\n",
        "where upconv2D can be implemented through conv2D, our\n",
        "set upconv can also be directly achieved with the same set\n",
        "conv layer as defined in Eq. (1), but with a different local region\n",
        "sampling strategy. Instead of using farthest point sampling, they compute features on specified locations by the target points.\n",
        "\n",
        "```\n",
        "class SetUpConv(nn.Module):\n",
        "    def __init__(self, num_samples, in_channels1, in_channels2, out_channels1, out_channels2):\n",
        "        super(SetUpConv, self).__init__()\n",
        "        \n",
        "        self.group = Group(None, num_samples, knn=True)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels1 = [in_channels1+3, *out_channels1]\n",
        "        for i in range(1, len(out_channels1)):\n",
        "            layers += [nn.Conv2d(out_channels1[i - 1], out_channels1[i], 1, bias=True), nn.BatchNorm2d(out_channels1[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv1 = nn.Sequential(*layers)\n",
        "        \n",
        "        layers = []\n",
        "        if len(out_channels1) == 1:\n",
        "            out_channels2 = [in_channels1+in_channels2+3, *out_channels2]\n",
        "        else:\n",
        "            out_channels2 = [out_channels1[-1]+in_channels2, *out_channels2]\n",
        "        for i in range(1, len(out_channels2)):\n",
        "            layers += [nn.Conv2d(out_channels2[i - 1], out_channels2[i], 1, bias=True), nn.BatchNorm2d(out_channels2[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv2 = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        new_features = self.group(points1, points2, features1)\n",
        "        new_features = self.conv1(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        new_features = torch.cat([new_features, features2], dim=1)\n",
        "        new_features = new_features.unsqueeze(3)\n",
        "        new_features = self.conv2(new_features)\n",
        "        new_features = new_features.squeeze(3)\n",
        "        return new_features\n",
        "\n",
        "class FeaturePropagation(nn.Module):\n",
        "    def __init__(self, in_channels1, in_channels2, out_channels):\n",
        "        super(FeaturePropagation, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [in_channels1+in_channels2, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        dist, ind = three_nn(points2.permute(0, 2, 1).contiguous(), points1.permute(0, 2, 1).contiguous())\n",
        "        dist = dist * dist\n",
        "        dist[dist < 1e-10] = 1e-10\n",
        "        inverse_dist = 1.0 / dist\n",
        "        norm = torch.sum(inverse_dist, dim=2, keepdim=True)\n",
        "        weights = inverse_dist / norm\n",
        "        #new_features = three_interpolate(features1, ind, weights) # wrong gradients\n",
        "        new_features = torch.sum(group_gather_by_index(features1, ind) * weights.unsqueeze(1), dim = 3)\n",
        "        new_features = torch.cat([new_features, features2], dim=1)\n",
        "        new_features = self.conv(new_features.unsqueeze(3)).squeeze(3)\n",
        "        return new_features\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpn0O0oK2KTW"
      },
      "source": [
        "## 4.4. Network Architecture\n",
        "\n",
        "The final FlowNet3D architecture is composed of four\n",
        "set conv layers, one flow embedding layer and four set upconv\n",
        "layers (corresponding to the four set conv layers) and\n",
        "a final linear flow regression layer that outputs the R3 predicted\n",
        "scene flow.\n",
        "\n",
        "<img align=\"center\" width=\"800\" height=\"300\" src=\"https://drive.google.com/uc?id=1QfeVo0GvSmt4xiBet9sPEvsSI_rxjQIG\">\n",
        "\n",
        "```\n",
        "class FlowNet3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlowNet3D, self).__init__()\n",
        "        \n",
        "        self.set_conv1 = SetConv(1024, 0.5, 16, 3, [32, 32, 64])\n",
        "        self.set_conv2 = SetConv(256, 1.0, 16, 64, [64, 64, 128])\n",
        "        self.flow_embedding = FlowEmbedding(64, 128, [128, 128, 128])\n",
        "        self.set_conv3 = SetConv(64, 2.0, 8, 128, [128, 128, 256])\n",
        "        self.set_conv4 = SetConv(16, 4.0, 8, 256, [256, 256, 512])\n",
        "        self.set_upconv1 = SetUpConv(8, 512, 256, [], [256, 256])\n",
        "        self.set_upconv2 = SetUpConv(8, 256, 256, [128, 128, 256], [256])\n",
        "        self.set_upconv3 = SetUpConv(8, 256, 64, [128, 128, 256], [256])\n",
        "        self.fp = FeaturePropagation(256, 3, [256, 256])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv1d(256, 128, 1, bias=True),\n",
        "            nn.BatchNorm1d(128, eps=0.001),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 3, 1, bias=True)\n",
        "        )\n",
        "         \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        points1_1, features1_1 = self.set_conv1(points1, features1)\n",
        "        points1_2, features1_2 = self.set_conv2(points1_1, features1_1)\n",
        "\n",
        "        points2_1, features2_1 = self.set_conv1(points2, features2)\n",
        "        points2_2, features2_2 = self.set_conv2(points2_1, features2_1)\n",
        "\n",
        "        embedding = self.flow_embedding(points1_2, points2_2, features1_2, features2_2)\n",
        "        \n",
        "        points1_3, features1_3 = self.set_conv3(points1_2, embedding)\n",
        "        points1_4, features1_4 = self.set_conv4(points1_3, features1_3)\n",
        "        \n",
        "        new_features1_3 = self.set_upconv1(points1_4, points1_3, features1_4, features1_3)\n",
        "        new_features1_2 = self.set_upconv2(points1_3, points1_2, new_features1_3, torch.cat([features1_2, embedding], dim=1))\n",
        "        new_features1_1 = self.set_upconv3(points1_2, points1_1, new_features1_2, features1_1)\n",
        "        new_features1 = self.fp(points1_1, points1, new_features1_1, features1)\n",
        "\n",
        "        flow = self.classifier(new_features1)\n",
        "        \n",
        "        return flow\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPJsYiWYBYt"
      },
      "source": [
        "## Training loss\n",
        "They use smooth L1 loss (huber loss) for scene flow supervision,\n",
        "together with a cycle-consistency regularization.\n",
        "\n",
        "Given a point cloud $\\mathcal{P}=\\left\\{x_{i}\\right\\}_{i=1}^{n_{1}}$ at frame $t$ and a point cloud $\\mathcal{Q}=\\left\\{y_{j}\\right\\}_{j=1}^{n_{2}}$ at frame $t+1,$ the network predicts scene flow as $\\mathcal{D}=F(\\mathcal{P}, \\mathcal{Q} ; \\Theta)=\\left\\{d_{i}\\right\\}_{i=1}^{n_{1}}$ where $F$ is the\n",
        "FlowNet3D model with parameters $\\Theta$. With ground truth scene flow $\\mathcal{D}^{*}=\\left\\{d_{i}^{*}\\right\\}_{i=1}^{n_{1}},$ our loss is defined as shown here. In the equation, $\\left\\|d_{i}^{\\prime}+d_{i}\\right\\|$ is the cycle-consistency term that enforces the backward flow $\\left\\{d_{i}^{\\prime}\\right\\}_{i=1}^{n_{1}}=F\\left(\\mathcal{P}^{\\prime}, \\mathcal{P} ; \\Theta\\right)$ from\n",
        "the shifted point cloud $\\mathcal{P}^{\\prime}=\\left\\{x_{i}+d_{i}\\right\\}_{i=1}^{n_{1}}$ to the original point cloud $\\mathcal{P}$ is close to the reverse of the forward flow\n",
        "\n",
        "$$\n",
        "L\\left(\\mathcal{P}, \\mathcal{Q}, \\mathcal{D}^{*}, \\Theta\\right)=\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}}\\left\\{\\left\\|d_{i}-d_{i}^{*}\\right\\|+\\lambda\\left\\|d_{i}^{\\prime}+d_{i}\\right\\|\\right\\}\n",
        "$$\n",
        "\n",
        "But the implemented loss is just a L2 loss:\n",
        "\n",
        "```\n",
        "def criterion(pred_flow, flow, mask):\n",
        "    loss = torch.mean(mask * torch.sum((pred_flow - flow) * (pred_flow - flow), dim=1) / 2.0)\n",
        "    return loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhoEmesfgkD-"
      },
      "source": [
        "## Part B\n",
        "\n",
        "In the second part, you will extend the code to KITTI. You can use the models pre-trained on FlyingThings3D in the `models` folder. For KITTI, please use the preprocessed data by removing ground from the original TensorFlow repo:\n",
        "\n",
        "\"We release the processed KITTI scene flow dataset [here](https://drive.google.com/open?id=1XBsF35wKY0rmaL7x7grD_evvKCAccbKi) for download (total size ~266MB). The KITTI scene flow dataset was processed by converting the 2D optical flow into 3D scene flow and removing the ground points. We processed the first 150 data points from KITTI scene flow dataset. Each of the data points are stored as a .npz file and its dictionary has three keys: pos1, pos2 and gt, representing the first frame of point cloud, second frame of point cloud and the ground truth scene flow vectors for the points in the first frame.\"\n",
        "\n",
        "**Question**: Using the model trained on FlyingThings3D and evaluating it on the pre-processed KITTI, can you reproduce the results in the last row of the Table 4? You will need to implement the 'outliers' metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFhBeZUE0RsS"
      },
      "source": [
        "**Answer**: We have found 2% difference in EPE and 6% difference in the outlier metric. The two main reasons are the difference in the loss function and the lack of colored features in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyV62XoPgkD_"
      },
      "source": [
        "### Hints\n",
        "For running the code you will need to install kaolin v0.1.\n",
        "You can use the following lines to install Kaolin on Google Colab, or you can follow the [installation guide](https://kaolin.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJ8DLyDgkD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db148289-f596-4efc-9c0d-3b02a60e547c"
      },
      "source": [
        "!git clone -b v0.1 https://github.com/NVIDIAGameWorks/kaolin.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'kaolin' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-nqV0nDgkEA"
      },
      "source": [
        "# !pip install kaolin/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uul5TJ75gkEA"
      },
      "source": [
        "The installation might take a couple of minutes. After it is done, restart the runtime on Google on Colab. After restarting, Kaolin should be installed correctly.\n",
        "\n",
        "You will also need to the [flownet3d.pytorch](https://github.com/multimodallearning/flownet3d.pytorch) repository, which you can clone:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWrGSwp2gkEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df003f93-7c38-4b92-c8c2-9a3f6725adec"
      },
      "source": [
        "!git clone https://github.com/multimodallearning/flownet3d.pytorch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'flownet3d.pytorch' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpLXhpEIgkEB"
      },
      "source": [
        "You can use `gdown` for downloading the [KITTI data](https://drive.google.com/open?id=1XBsF35wKY0rmaL7x7grD_evvKCAccbKi):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnpwLENRgkEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60a7e55-246a-46db-becc-d13a5cc76a69"
      },
      "source": [
        "!gdown --id 1XBsF35wKY0rmaL7x7grD_evvKCAccbKi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XBsF35wKY0rmaL7x7grD_evvKCAccbKi\n",
            "To: /content/kitti_rm_ground.tar\n",
            "279MB [00:02, 99.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj1UgDcVk_N8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f312422-efb1-4e44-e514-ff0854697312"
      },
      "source": [
        "!tar -xvf kitti_rm_ground.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kitti_rm_ground/\n",
            "kitti_rm_ground/000054.npz\n",
            "kitti_rm_ground/000080.npz\n",
            "kitti_rm_ground/000091.npz\n",
            "kitti_rm_ground/000147.npz\n",
            "kitti_rm_ground/000024.npz\n",
            "kitti_rm_ground/000104.npz\n",
            "kitti_rm_ground/000035.npz\n",
            "kitti_rm_ground/000144.npz\n",
            "kitti_rm_ground/000119.npz\n",
            "kitti_rm_ground/000026.npz\n",
            "kitti_rm_ground/000051.npz\n",
            "kitti_rm_ground/000131.npz\n",
            "kitti_rm_ground/000052.npz\n",
            "kitti_rm_ground/000095.npz\n",
            "kitti_rm_ground/000060.npz\n",
            "kitti_rm_ground/000084.npz\n",
            "kitti_rm_ground/000077.npz\n",
            "kitti_rm_ground/000089.npz\n",
            "kitti_rm_ground/000048.npz\n",
            "kitti_rm_ground/000097.npz\n",
            "kitti_rm_ground/000120.npz\n",
            "kitti_rm_ground/000082.npz\n",
            "kitti_rm_ground/000129.npz\n",
            "kitti_rm_ground/000002.npz\n",
            "kitti_rm_ground/000088.npz\n",
            "kitti_rm_ground/000010.npz\n",
            "kitti_rm_ground/000116.npz\n",
            "kitti_rm_ground/000006.npz\n",
            "kitti_rm_ground/000014.npz\n",
            "kitti_rm_ground/000069.npz\n",
            "kitti_rm_ground/000071.npz\n",
            "kitti_rm_ground/000105.npz\n",
            "kitti_rm_ground/000058.npz\n",
            "kitti_rm_ground/000008.npz\n",
            "kitti_rm_ground/000124.npz\n",
            "kitti_rm_ground/000036.npz\n",
            "kitti_rm_ground/000086.npz\n",
            "kitti_rm_ground/000056.npz\n",
            "kitti_rm_ground/000027.npz\n",
            "kitti_rm_ground/000033.npz\n",
            "kitti_rm_ground/000114.npz\n",
            "kitti_rm_ground/000029.npz\n",
            "kitti_rm_ground/000013.npz\n",
            "kitti_rm_ground/000070.npz\n",
            "kitti_rm_ground/000068.npz\n",
            "kitti_rm_ground/000004.npz\n",
            "kitti_rm_ground/000136.npz\n",
            "kitti_rm_ground/000030.npz\n",
            "kitti_rm_ground/000100.npz\n",
            "kitti_rm_ground/000138.npz\n",
            "kitti_rm_ground/000050.npz\n",
            "kitti_rm_ground/000081.npz\n",
            "kitti_rm_ground/000063.npz\n",
            "kitti_rm_ground/000101.npz\n",
            "kitti_rm_ground/000117.npz\n",
            "kitti_rm_ground/000067.npz\n",
            "kitti_rm_ground/000028.npz\n",
            "kitti_rm_ground/000049.npz\n",
            "kitti_rm_ground/000130.npz\n",
            "kitti_rm_ground/000142.npz\n",
            "kitti_rm_ground/000053.npz\n",
            "kitti_rm_ground/000023.npz\n",
            "kitti_rm_ground/000075.npz\n",
            "kitti_rm_ground/000032.npz\n",
            "kitti_rm_ground/000076.npz\n",
            "kitti_rm_ground/000065.npz\n",
            "kitti_rm_ground/000057.npz\n",
            "kitti_rm_ground/000133.npz\n",
            "kitti_rm_ground/000137.npz\n",
            "kitti_rm_ground/000109.npz\n",
            "kitti_rm_ground/000087.npz\n",
            "kitti_rm_ground/000113.npz\n",
            "kitti_rm_ground/000039.npz\n",
            "kitti_rm_ground/000016.npz\n",
            "kitti_rm_ground/000017.npz\n",
            "kitti_rm_ground/000046.npz\n",
            "kitti_rm_ground/000149.npz\n",
            "kitti_rm_ground/000145.npz\n",
            "kitti_rm_ground/000043.npz\n",
            "kitti_rm_ground/000062.npz\n",
            "kitti_rm_ground/000106.npz\n",
            "kitti_rm_ground/000031.npz\n",
            "kitti_rm_ground/000061.npz\n",
            "kitti_rm_ground/000143.npz\n",
            "kitti_rm_ground/000098.npz\n",
            "kitti_rm_ground/000092.npz\n",
            "kitti_rm_ground/000093.npz\n",
            "kitti_rm_ground/000015.npz\n",
            "kitti_rm_ground/000074.npz\n",
            "kitti_rm_ground/000034.npz\n",
            "kitti_rm_ground/000059.npz\n",
            "kitti_rm_ground/000018.npz\n",
            "kitti_rm_ground/000040.npz\n",
            "kitti_rm_ground/000099.npz\n",
            "kitti_rm_ground/000001.npz\n",
            "kitti_rm_ground/000000.npz\n",
            "kitti_rm_ground/000044.npz\n",
            "kitti_rm_ground/000090.npz\n",
            "kitti_rm_ground/000096.npz\n",
            "kitti_rm_ground/000115.npz\n",
            "kitti_rm_ground/000146.npz\n",
            "kitti_rm_ground/000139.npz\n",
            "kitti_rm_ground/000047.npz\n",
            "kitti_rm_ground/000102.npz\n",
            "kitti_rm_ground/000125.npz\n",
            "kitti_rm_ground/000083.npz\n",
            "kitti_rm_ground/000011.npz\n",
            "kitti_rm_ground/000110.npz\n",
            "kitti_rm_ground/000041.npz\n",
            "kitti_rm_ground/000108.npz\n",
            "kitti_rm_ground/000019.npz\n",
            "kitti_rm_ground/000122.npz\n",
            "kitti_rm_ground/000078.npz\n",
            "kitti_rm_ground/000072.npz\n",
            "kitti_rm_ground/000037.npz\n",
            "kitti_rm_ground/000021.npz\n",
            "kitti_rm_ground/000045.npz\n",
            "kitti_rm_ground/000079.npz\n",
            "kitti_rm_ground/000042.npz\n",
            "kitti_rm_ground/000020.npz\n",
            "kitti_rm_ground/000012.npz\n",
            "kitti_rm_ground/000127.npz\n",
            "kitti_rm_ground/000003.npz\n",
            "kitti_rm_ground/000022.npz\n",
            "kitti_rm_ground/000107.npz\n",
            "kitti_rm_ground/000121.npz\n",
            "kitti_rm_ground/000005.npz\n",
            "kitti_rm_ground/000148.npz\n",
            "kitti_rm_ground/000140.npz\n",
            "kitti_rm_ground/000038.npz\n",
            "kitti_rm_ground/000055.npz\n",
            "kitti_rm_ground/000135.npz\n",
            "kitti_rm_ground/000025.npz\n",
            "kitti_rm_ground/000073.npz\n",
            "kitti_rm_ground/000126.npz\n",
            "kitti_rm_ground/000123.npz\n",
            "kitti_rm_ground/000112.npz\n",
            "kitti_rm_ground/000118.npz\n",
            "kitti_rm_ground/000009.npz\n",
            "kitti_rm_ground/000111.npz\n",
            "kitti_rm_ground/000085.npz\n",
            "kitti_rm_ground/000103.npz\n",
            "kitti_rm_ground/000094.npz\n",
            "kitti_rm_ground/000128.npz\n",
            "kitti_rm_ground/000141.npz\n",
            "kitti_rm_ground/000134.npz\n",
            "kitti_rm_ground/000132.npz\n",
            "kitti_rm_ground/000007.npz\n",
            "kitti_rm_ground/000066.npz\n",
            "kitti_rm_ground/000064.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0sCgU3IgkEC"
      },
      "source": [
        "Now you can try extending and running the code for KITTI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nblAfqm3gkEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6acca3-c62e-41e7-bac5-e4eb7893450a"
      },
      "source": [
        "import glob\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Manager\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import kaolin as kal\n",
        "from kaolin.models.PointNet2 import furthest_point_sampling\n",
        "from kaolin.models.PointNet2 import fps_gather_by_index\n",
        "from kaolin.models.PointNet2 import ball_query\n",
        "from kaolin.models.PointNet2 import group_gather_by_index\n",
        "from kaolin.models.PointNet2 import three_nn\n",
        "from kaolin.models.PointNet2 import three_interpolate\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/kaolin/datasets/__init__.py:8: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
            "    handler_func(fileobj, events)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 451, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 434, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
            "    self.events.trigger('post_run_cell')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/_event_manager.py\", line 28, in trigger\n",
            "    func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/pylab/backend_inline.py\", line 164, in configure_once\n",
            "    activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 1425, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/backends/__init__.py\", line 17, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaolin/datasets/__init__.py\", line 11, in <module>\n",
            "    from .nusc import NuscDetection\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaolin/datasets/nusc.py\", line 21, in <module>\n",
            "    from nuscenes.utils.geometry_utils import transform_matrix\n",
            "ModuleNotFoundError: No module named 'nuscenes'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: unable to import datasets/nusc:\n",
            "   No module named 'nuscenes'\n",
            "Warning: unable to import datasets/nusc:\n",
            "   None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uys2S0oEgkED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05eff473-a80f-4f02-da87-6068d1bcaae7"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self):\n",
        "      self.dir = './kitti_rm_ground/'\n",
        "      self.files = os.listdir(self.dir)\n",
        "\n",
        "    def __getitem__(self, index, npoints=2048):\n",
        "      data = np.load(self.dir + self.files[index])\n",
        "      pos1 = data['pos1'].astype('float32')\n",
        "      pos2 = data['pos2'].astype('float32')\n",
        "      flow = data['gt'].astype('float32')\n",
        "\n",
        "      pos1_center = np.mean(pos1, 0)\n",
        "      pos1 -= pos1_center\n",
        "      pos2 -= pos1_center\n",
        "        \n",
        "      pos1 = torch.from_numpy(pos1[:npoints]).t()\n",
        "      pos2 = torch.from_numpy(pos2[:npoints]).t()\n",
        "      flow = torch.from_numpy(flow[:npoints]).t()\n",
        "\n",
        "      return pos1, pos2, flow\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "test_set = TestDataset()\n",
        "points1, points2, flow = test_set[0]\n",
        "\n",
        "print(points1.shape, points1.dtype)\n",
        "print(points2.shape, points2.dtype)\n",
        "print(flow.shape, flow.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2048]) torch.float32\n",
            "torch.Size([3, 2048]) torch.float32\n",
            "torch.Size([3, 2048]) torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsNe5lXugkEE"
      },
      "source": [
        "def set_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def pdist2squared(x, y):\n",
        "    xx = (x**2).sum(dim=1).unsqueeze(2)\n",
        "    yy = (y**2).sum(dim=1).unsqueeze(1)\n",
        "    dist = xx + yy - 2.0 * torch.bmm(x.permute(0, 2, 1), y)\n",
        "    dist[dist != dist] = 0\n",
        "    dist = torch.clamp(dist, 0.0, np.inf)\n",
        "    return dist\n",
        "\n",
        "def parameter_count(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class ClippedStepLR(optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, step_size, min_lr, gamma=0.1, last_epoch=-1):\n",
        "        self.step_size = step_size\n",
        "        self.min_lr = min_lr\n",
        "        self.gamma = gamma\n",
        "        super(ClippedStepLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [max(base_lr * self.gamma ** (self.last_epoch // self.step_size), self.min_lr)\n",
        "                for base_lr in self.base_lrs]\n",
        "    \n",
        "def criterion(pred_flow, flow, mask):\n",
        "    loss = torch.mean(mask * torch.sum((pred_flow - flow) * (pred_flow - flow), dim=1) / 2.0)\n",
        "    return loss\n",
        "\n",
        "def error(pred, labels, mask):\n",
        "    pred = pred.permute(0,2,1).cpu().numpy()\n",
        "    labels = labels.permute(0,2,1).cpu().numpy()\n",
        "    mask = mask.cpu().numpy()\n",
        "    \n",
        "    err = np.sqrt(np.sum((pred - labels)**2, 2) + 1e-20)\n",
        "\n",
        "    gtflow_len = np.sqrt(np.sum(labels*labels, 2) + 1e-20) # B,N\n",
        "    acc050 = np.sum(np.logical_or((err <= 0.05)*mask, (err/gtflow_len <= 0.05)*mask), axis=1)\n",
        "    acc010 = np.sum(np.logical_or((err <= 0.1)*mask, (err/gtflow_len <= 0.1)*mask), axis=1)\n",
        "\n",
        "    mask_sum = np.sum(mask, 1)\n",
        "    acc050 = acc050[mask_sum > 0] / mask_sum[mask_sum > 0]\n",
        "    acc050 = np.mean(acc050)\n",
        "    acc010 = acc010[mask_sum > 0] / mask_sum[mask_sum > 0]\n",
        "    acc010 = np.mean(acc010)\n",
        "\n",
        "    epe = np.sum(err * mask, 1)[mask_sum > 0] / mask_sum[mask_sum > 0]\n",
        "    epe = np.mean(epe)\n",
        "    return epe, acc050, acc010"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbOmymoJgkEF"
      },
      "source": [
        "from kaolin.models.PointNet2 import furthest_point_sampling\n",
        "from kaolin.models.PointNet2 import fps_gather_by_index\n",
        "\n",
        "class Sample(nn.Module):\n",
        "    def __init__(self, num_points):\n",
        "        super(Sample, self).__init__()\n",
        "        \n",
        "        self.num_points = num_points\n",
        "        \n",
        "    def forward(self, points):\n",
        "        new_points_ind = furthest_point_sampling(points.permute(0, 2, 1).contiguous(), self.num_points)\n",
        "        new_points = fps_gather_by_index(points, new_points_ind)\n",
        "        return new_points\n",
        "    \n",
        "class Group(nn.Module):\n",
        "    def __init__(self, radius, num_samples, knn=False):\n",
        "        super(Group, self).__init__()\n",
        "        \n",
        "        self.radius = radius\n",
        "        self.num_samples = num_samples\n",
        "        self.knn = knn\n",
        "        \n",
        "    def forward(self, points, new_points, features):\n",
        "        if self.knn:\n",
        "            dist = pdist2squared(points, new_points)\n",
        "            ind = dist.topk(self.num_samples, dim=1, largest=False)[1].int().permute(0, 2, 1).contiguous()\n",
        "        else:\n",
        "            ind = ball_query(self.radius, self.num_samples, points.permute(0, 2, 1).contiguous(),\n",
        "                             new_points.permute(0, 2, 1).contiguous(), False)\n",
        "        grouped_points = group_gather_by_index(points, ind)\n",
        "        grouped_points -= new_points.unsqueeze(3)\n",
        "        grouped_features = group_gather_by_index(features, ind)\n",
        "        new_features = torch.cat([grouped_points, grouped_features], dim=1)\n",
        "        return new_features\n",
        "\n",
        "class SetConv(nn.Module):\n",
        "    def __init__(self, num_points, radius, num_samples, in_channels, out_channels):\n",
        "        super(SetConv, self).__init__()\n",
        "        \n",
        "        self.sample = Sample(num_points)\n",
        "        self.group = Group(radius, num_samples)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [in_channels+3, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points, features):\n",
        "        new_points = self.sample(points)\n",
        "        new_features = self.group(points, new_points, features)\n",
        "        new_features = self.conv(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        return new_points, new_features\n",
        "    \n",
        "class FlowEmbedding(nn.Module):\n",
        "    def __init__(self, num_samples, in_channels, out_channels):\n",
        "        super(FlowEmbedding, self).__init__()\n",
        "        \n",
        "        self.num_samples = num_samples\n",
        "        \n",
        "        self.group = Group(None, self.num_samples, knn=True)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [2*in_channels+3, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        new_features = self.group(points2, points1, features2)\n",
        "        new_features = torch.cat([new_features, features1.unsqueeze(3).expand(-1, -1, -1, self.num_samples)], dim=1)\n",
        "        new_features = self.conv(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        return new_features\n",
        "    \n",
        "class SetUpConv(nn.Module):\n",
        "    def __init__(self, num_samples, in_channels1, in_channels2, out_channels1, out_channels2):\n",
        "        super(SetUpConv, self).__init__()\n",
        "        \n",
        "        self.group = Group(None, num_samples, knn=True)\n",
        "        \n",
        "        layers = []\n",
        "        out_channels1 = [in_channels1+3, *out_channels1]\n",
        "        for i in range(1, len(out_channels1)):\n",
        "            layers += [nn.Conv2d(out_channels1[i - 1], out_channels1[i], 1, bias=True), nn.BatchNorm2d(out_channels1[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv1 = nn.Sequential(*layers)\n",
        "        \n",
        "        layers = []\n",
        "        if len(out_channels1) == 1:\n",
        "            out_channels2 = [in_channels1+in_channels2+3, *out_channels2]\n",
        "        else:\n",
        "            out_channels2 = [out_channels1[-1]+in_channels2, *out_channels2]\n",
        "        for i in range(1, len(out_channels2)):\n",
        "            layers += [nn.Conv2d(out_channels2[i - 1], out_channels2[i], 1, bias=True), nn.BatchNorm2d(out_channels2[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv2 = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        new_features = self.group(points1, points2, features1)\n",
        "        new_features = self.conv1(new_features)\n",
        "        new_features = new_features.max(dim=3)[0]\n",
        "        new_features = torch.cat([new_features, features2], dim=1)\n",
        "        new_features = new_features.unsqueeze(3)\n",
        "        new_features = self.conv2(new_features)\n",
        "        new_features = new_features.squeeze(3)\n",
        "        return new_features\n",
        "    \n",
        "class FeaturePropagation(nn.Module):\n",
        "    def __init__(self, in_channels1, in_channels2, out_channels):\n",
        "        super(FeaturePropagation, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        out_channels = [in_channels1+in_channels2, *out_channels]\n",
        "        for i in range(1, len(out_channels)):\n",
        "            layers += [nn.Conv2d(out_channels[i - 1], out_channels[i], 1, bias=True), nn.BatchNorm2d(out_channels[i], eps=0.001), nn.ReLU()]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        dist, ind = three_nn(points2.permute(0, 2, 1).contiguous(), points1.permute(0, 2, 1).contiguous())\n",
        "        dist = dist * dist\n",
        "        dist[dist < 1e-10] = 1e-10\n",
        "        inverse_dist = 1.0 / dist\n",
        "        norm = torch.sum(inverse_dist, dim=2, keepdim=True)\n",
        "        weights = inverse_dist / norm\n",
        "        #new_features = three_interpolate(features1, ind, weights) # wrong gradients\n",
        "        new_features = torch.sum(group_gather_by_index(features1, ind) * weights.unsqueeze(1), dim = 3)\n",
        "        new_features = torch.cat([new_features, features2], dim=1)\n",
        "        new_features = self.conv(new_features.unsqueeze(3)).squeeze(3)\n",
        "        return new_features\n",
        "\n",
        "class FlowNet3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlowNet3D, self).__init__()\n",
        "        \n",
        "        self.set_conv1 = SetConv(1024, 0.5, 16, 3, [32, 32, 64])\n",
        "        self.set_conv2 = SetConv(256, 1.0, 16, 64, [64, 64, 128])\n",
        "        self.flow_embedding = FlowEmbedding(64, 128, [128, 128, 128])\n",
        "        self.set_conv3 = SetConv(64, 2.0, 8, 128, [128, 128, 256])\n",
        "        self.set_conv4 = SetConv(16, 4.0, 8, 256, [256, 256, 512])\n",
        "        self.set_upconv1 = SetUpConv(8, 512, 256, [], [256, 256])\n",
        "        self.set_upconv2 = SetUpConv(8, 256, 256, [128, 128, 256], [256])\n",
        "        self.set_upconv3 = SetUpConv(8, 256, 64, [128, 128, 256], [256])\n",
        "        self.fp = FeaturePropagation(256, 3, [256, 256])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv1d(256, 128, 1, bias=True),\n",
        "            nn.BatchNorm1d(128, eps=0.001),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 3, 1, bias=True)\n",
        "        )\n",
        "         \n",
        "    def forward(self, points1, points2, features1, features2):\n",
        "        points1_1, features1_1 = self.set_conv1(points1, features1)\n",
        "        points1_2, features1_2 = self.set_conv2(points1_1, features1_1)\n",
        "\n",
        "        points2_1, features2_1 = self.set_conv1(points2, features2)\n",
        "        points2_2, features2_2 = self.set_conv2(points2_1, features2_1)\n",
        "\n",
        "        embedding = self.flow_embedding(points1_2, points2_2, features1_2, features2_2)\n",
        "        \n",
        "        points1_3, features1_3 = self.set_conv3(points1_2, embedding)\n",
        "        points1_4, features1_4 = self.set_conv4(points1_3, features1_3)\n",
        "        \n",
        "        new_features1_3 = self.set_upconv1(points1_4, points1_3, features1_4, features1_3)\n",
        "        new_features1_2 = self.set_upconv2(points1_3, points1_2, new_features1_3, torch.cat([features1_2, embedding], dim=1))\n",
        "        new_features1_1 = self.set_upconv3(points1_2, points1_1, new_features1_2, features1_1)\n",
        "        new_features1 = self.fp(points1_1, points1, new_features1_1, features1)\n",
        "\n",
        "        flow = self.classifier(new_features1)\n",
        "        \n",
        "        return flow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLUMI5vVgkEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c89135-17d5-4e74-dbb4-7b6558b3e7cb"
      },
      "source": [
        "%%time\n",
        "\n",
        "# data\n",
        "test_set = TestDataset()\n",
        "\n",
        "print('test set:', len(test_set))\n",
        "\n",
        "# model\n",
        "net = FlowNet3D().cuda()\n",
        "net.load_state_dict(torch.load('./flownet3d.pytorch/models/net.pth'))\n",
        "net.eval()\n",
        "\n",
        "# statistics\n",
        "loss_sum = 0\n",
        "epe_sum = 0\n",
        "acc050_sum = 0\n",
        "acc010_sum = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    # for each mini-batch\n",
        "    for points1, points2, flow in test_set:\n",
        "            \n",
        "        # to GPU\n",
        "        points1 = points1.unsqueeze(0).cuda().contiguous()\n",
        "        points2 = points2.unsqueeze(0).cuda().contiguous()\n",
        "        flow = flow.unsqueeze(0).cuda().contiguous()\n",
        "    \n",
        "        # forward\n",
        "        features1 = torch.zeros_like(points1)\n",
        "        features2 = torch.zeros_like(points2)\n",
        "        pred_flow = net(points1, points2, features1, features2)\n",
        "        \n",
        "        # statistics\n",
        "        mask1 = torch.ones_like(features1)\n",
        "        loss = criterion(pred_flow, flow, mask1)\n",
        "        loss_sum += loss.item()\n",
        "        epe, acc050, acc010 = error(pred_flow, flow, mask1)\n",
        "        epe_sum += epe\n",
        "        acc050_sum += acc050\n",
        "        acc010_sum += acc010\n",
        "        \n",
        "print('mean loss:', loss_sum/len(test_set))\n",
        "print('mean epe:', epe_sum/len(test_set))\n",
        "print('mean acc050:', acc050_sum/len(test_set))\n",
        "    \n",
        "print('---')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set: 150\n",
            "mean loss: 0.022397985982242973\n",
            "mean epe: 0.1567641403277715\n",
            "mean acc050: 0.12694986979166667\n",
            "---\n",
            "CPU times: user 6.42 s, sys: 1.48 s, total: 7.9 s\n",
            "Wall time: 7.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecpa_1a1ytfo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}